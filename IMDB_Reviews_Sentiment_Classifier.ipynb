{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOx5fUHgeJztZU5QBaoQfno",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/philmorrison/resources/blob/master/IMDB_Reviews_Sentiment_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz5k2DzHsEWp"
      },
      "source": [
        "import numpy as np\n",
        "import urllib\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, RNN, GRU, LSTM, Bidirectional "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtqQPodRuV4Q"
      },
      "source": [
        "(train_ds, test_ds), info = tfds.load(name=\"imdb_reviews\",     # imdb_reviews\n",
        "                 with_info=True,\n",
        "                 split=['train', 'test'],  # which sets to get\n",
        "                 as_supervised=True)  # ask for supervised tuples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65VHQARyvDdX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "43469694-f983-496b-eb05-d2c46a551e20"
      },
      "source": [
        "info"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tfds.core.DatasetInfo(\n",
              "    name='imdb_reviews',\n",
              "    version=1.0.0,\n",
              "    description='Large Movie Review Dataset.\n",
              "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.',\n",
              "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
              "    features=FeaturesDict({\n",
              "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
              "        'text': Text(shape=(), dtype=tf.string),\n",
              "    }),\n",
              "    total_num_examples=100000,\n",
              "    splits={\n",
              "        'test': 25000,\n",
              "        'train': 25000,\n",
              "        'unsupervised': 50000,\n",
              "    },\n",
              "    supervised_keys=('text', 'label'),\n",
              "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
              "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
              "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
              "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
              "      month     = {June},\n",
              "      year      = {2011},\n",
              "      address   = {Portland, Oregon, USA},\n",
              "      publisher = {Association for Computational Linguistics},\n",
              "      pages     = {142--150},\n",
              "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
              "    }\"\"\",\n",
              "    redistribution_info=,\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcXzI1ybv3nM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "db5713cf-55bb-4e6c-cfe8-29f35c3b91cf"
      },
      "source": [
        "print(type(train_ds))\n",
        "print(type(test_ds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter'>\n",
            "<class 'tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0En2alBvUZm"
      },
      "source": [
        "texts = []\n",
        "labels = []\n",
        "\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "\n",
        "for text, label in train_ds.take(25000):\n",
        "  # print(str(text.numpy())[1:])\n",
        "  # print(label.numpy())\n",
        "  review = str(text.numpy())[1:]\n",
        "  y = label.numpy()\n",
        "  texts.append(review)\n",
        "  labels.append(y)\n",
        "\n",
        "for text, label in test_ds.take(10000):\n",
        "  # print(str(text.numpy())[1:])\n",
        "  # print(label.numpy())\n",
        "  review = str(text.numpy())[1:]\n",
        "  y = label.numpy()\n",
        "  test_texts.append(review)\n",
        "  test_labels.append(y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjYdFGk52jTh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "0d1c1c5f-7be9-4493-eec0-222d05b9abe4"
      },
      "source": [
        "print(len(texts))\n",
        "print(len(labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000\n",
            "25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH4WKyaG2xP_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "706fb28d-837b-48f1-b29a-faba95bb715c"
      },
      "source": [
        "texts[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\"This was an absolutely terrible movie. Don\\'t be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie\\'s ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor\\'s like Christopher Walken\\'s good name. I could barely sit through it.\"'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fjgc2z1P24r6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "271d5a04-b2da-4ad4-a55a-bba54c86acb7"
      },
      "source": [
        "## Create Word Index and Reverse Index\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 10000, oov_token = '<UNK>')  # takes into account top 10000 words only\n",
        "tokenizer.fit_on_texts(texts)          # build the word index\n",
        "word2index = tokenizer.word_index        # recover the word index\n",
        "index2word = {value: key for key, value in word2index.items()  } # create reverse index\n",
        "\n",
        "print(word2index)\n",
        "print(index2word)\n",
        "print(len(word2index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsK0qZEk3mR6"
      },
      "source": [
        "## Fit tokenizer to documents\n",
        "train_sequences = tokenizer.texts_to_sequences(texts) # tokenize sentences into their index numbers\n",
        "train_sequences[0]\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcUP2Y4L4E3m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "72944cff-8227-495e-cb84-f87117285426"
      },
      "source": [
        "## Use padding to ensure all sequences are the same length\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "maxlen = 300           # cut off for num tokens in each example\n",
        "x_train = sequence.pad_sequences(train_sequences,\n",
        "                                 padding='post', # add padding to the end, 'pre' for before\n",
        "                                 truncating='post', # remove values from sequences larger than maxlen, either at the beginning 'pre' or at the end 'post' of the sequences\n",
        "                                 maxlen=maxlen) # turn lists of integers into tensor of shape(samples, maxlen)\n",
        "x_test = sequence.pad_sequences(test_sequences, maxlen=maxlen) # turn lists of integers into tensor of shape(samples, maxlen)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(x_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 300)\n",
            "(10000, 300)\n",
            "[  12   14   35  438  399   18  173   29    1    9   33 1376 3399   42\n",
            "  496    1  196   25   87  155   19   12  210  339   29   69  247  212\n",
            "    9  486   61   69   87  115   98   24 5741   12 3315  658  776   12\n",
            "   18    7   35  405 8229  177 2476  425    2   91 1251  139   71  148\n",
            "   55    2    1 7526   71  228   69 2960   16    1 2880    1    1 1505\n",
            " 4997    3   40 3949  118 1606   17 3399   14  162   19    4 1251  926\n",
            " 7988    9    4   18   13   14 4199    5  101  147 1235   11  239  693\n",
            "   13   45   25  100   39   12 7235    1   39 1376    1   52  408   11\n",
            "   98 1212  873  144   10    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnkyVy9o5L67"
      },
      "source": [
        "labels = np.array(labels)\n",
        "test_labels = np.array(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ta7SELaV5viv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c8778a4e-9cce-42d4-9d40-1fcdc2c310ab"
      },
      "source": [
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkpMkMf65yQF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "ef41b0f5-37ab-4f23-a389-e871ab0efc64"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(10000, 64, input_length = maxlen))\n",
        "model.add(GRU(64, recurrent_dropout=0.2))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 300, 64)           640000    \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 64)                24960     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 667,073\n",
            "Trainable params: 667,073\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mar_UNvoKICr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f94b2ee3-5675-47c6-8f4b-8783d53a20a4"
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "model.fit(x_train, labels,\n",
        "          epochs=30,\n",
        "          batch_size=128,\n",
        "          verbose=1,\n",
        "          validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "157/157 [==============================] - 88s 562ms/step - loss: 0.6932 - acc: 0.5026 - val_loss: 0.6926 - val_acc: 0.5062\n",
            "Epoch 2/30\n",
            "157/157 [==============================] - 86s 546ms/step - loss: 0.6985 - acc: 0.5396 - val_loss: 0.6856 - val_acc: 0.5216\n",
            "Epoch 3/30\n",
            "157/157 [==============================] - 86s 546ms/step - loss: 0.6588 - acc: 0.5839 - val_loss: 0.6652 - val_acc: 0.5580\n",
            "Epoch 4/30\n",
            "157/157 [==============================] - 86s 548ms/step - loss: 0.5802 - acc: 0.7238 - val_loss: 0.5362 - val_acc: 0.7682\n",
            "Epoch 5/30\n",
            "157/157 [==============================] - 86s 547ms/step - loss: 0.5576 - acc: 0.7305 - val_loss: 0.6843 - val_acc: 0.5498\n",
            "Epoch 6/30\n",
            "157/157 [==============================] - 86s 547ms/step - loss: 0.5224 - acc: 0.7636 - val_loss: 0.4885 - val_acc: 0.8060\n",
            "Epoch 7/30\n",
            "157/157 [==============================] - 86s 548ms/step - loss: 0.4597 - acc: 0.8156 - val_loss: 0.6900 - val_acc: 0.5448\n",
            "Epoch 8/30\n",
            "157/157 [==============================] - 86s 547ms/step - loss: 0.4497 - acc: 0.8157 - val_loss: 0.4811 - val_acc: 0.8158\n",
            "Epoch 9/30\n",
            "157/157 [==============================] - 86s 546ms/step - loss: 0.4429 - acc: 0.8258 - val_loss: 0.5145 - val_acc: 0.7738\n",
            "Epoch 10/30\n",
            "157/157 [==============================] - 85s 544ms/step - loss: 0.4300 - acc: 0.8127 - val_loss: 0.6678 - val_acc: 0.5776\n",
            "Epoch 11/30\n",
            "157/157 [==============================] - 86s 548ms/step - loss: 0.4108 - acc: 0.8313 - val_loss: 0.4560 - val_acc: 0.8220\n",
            "Epoch 12/30\n",
            "157/157 [==============================] - 86s 546ms/step - loss: 0.4073 - acc: 0.8410 - val_loss: 0.4833 - val_acc: 0.8152\n",
            "Epoch 13/30\n",
            "157/157 [==============================] - 86s 545ms/step - loss: 0.3453 - acc: 0.8791 - val_loss: 0.4389 - val_acc: 0.8362\n",
            "Epoch 14/30\n",
            "157/157 [==============================] - 86s 545ms/step - loss: 0.3333 - acc: 0.8774 - val_loss: 0.4671 - val_acc: 0.8318\n",
            "Epoch 15/30\n",
            "157/157 [==============================] - 86s 546ms/step - loss: 0.3070 - acc: 0.8849 - val_loss: 0.5008 - val_acc: 0.8442\n",
            "Epoch 16/30\n",
            "157/157 [==============================] - 86s 545ms/step - loss: 0.2698 - acc: 0.9044 - val_loss: 0.4619 - val_acc: 0.8408\n",
            "Epoch 17/30\n",
            "157/157 [==============================] - 86s 547ms/step - loss: 0.2316 - acc: 0.9189 - val_loss: 0.4433 - val_acc: 0.8652\n",
            "Epoch 18/30\n",
            "157/157 [==============================] - 86s 545ms/step - loss: 0.1954 - acc: 0.9316 - val_loss: 0.4207 - val_acc: 0.8608\n",
            "Epoch 19/30\n",
            "157/157 [==============================] - 85s 544ms/step - loss: 0.1653 - acc: 0.9414 - val_loss: 0.4607 - val_acc: 0.8602\n",
            "Epoch 20/30\n",
            "157/157 [==============================] - 86s 545ms/step - loss: 0.1431 - acc: 0.9491 - val_loss: 0.3953 - val_acc: 0.8732\n",
            "Epoch 21/30\n",
            "157/157 [==============================] - 86s 545ms/step - loss: 0.1221 - acc: 0.9572 - val_loss: 0.3934 - val_acc: 0.8558\n",
            "Epoch 22/30\n",
            "157/157 [==============================] - 86s 549ms/step - loss: 0.1060 - acc: 0.9632 - val_loss: 0.4657 - val_acc: 0.8736\n",
            "Epoch 23/30\n",
            "157/157 [==============================] - 86s 546ms/step - loss: 0.0898 - acc: 0.9689 - val_loss: 0.5582 - val_acc: 0.8694\n",
            "Epoch 24/30\n",
            "157/157 [==============================] - 86s 545ms/step - loss: 0.0728 - acc: 0.9751 - val_loss: 0.5541 - val_acc: 0.8732\n",
            "Epoch 25/30\n",
            "157/157 [==============================] - 86s 547ms/step - loss: 0.0635 - acc: 0.9789 - val_loss: 0.5906 - val_acc: 0.8244\n",
            "Epoch 26/30\n",
            "157/157 [==============================] - 86s 547ms/step - loss: 0.0542 - acc: 0.9829 - val_loss: 0.6132 - val_acc: 0.8550\n",
            "Epoch 27/30\n",
            "157/157 [==============================] - 86s 546ms/step - loss: 0.0460 - acc: 0.9843 - val_loss: 0.5836 - val_acc: 0.8630\n",
            "Epoch 28/30\n",
            "157/157 [==============================] - 86s 546ms/step - loss: 0.0388 - acc: 0.9884 - val_loss: 0.8397 - val_acc: 0.8434\n",
            "Epoch 29/30\n",
            "157/157 [==============================] - 86s 547ms/step - loss: 0.0353 - acc: 0.9882 - val_loss: 0.7490 - val_acc: 0.8630\n",
            "Epoch 30/30\n",
            "157/157 [==============================] - 86s 545ms/step - loss: 0.0275 - acc: 0.9913 - val_loss: 0.7217 - val_acc: 0.8538\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fca7d940e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTJNQ2u8Wcv_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "762640ea-63c3-49fd-ebcd-af84bad47f8a"
      },
      "source": [
        "model.evaluate(x_test, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 12s 38ms/step - loss: 0.9705 - acc: 0.7814\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9705485105514526, 0.7814000248908997]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}